"""
APPRENTISSAGE PAR RENFORCEMENT - Q-LEARNING
==========================================

Exemple complet avec un environnement GridWorld simple o√π un agent
doit apprendre √† naviguer vers un tr√©sor en √©vitant des obstacles.
"""

import numpy as np
import matplotlib.pyplot as plt
import random
from collections import defaultdict

# =========================================
# PARTIE 1 : ENVIRONNEMENT GRIDWORLD
# =========================================

class GridWorld:
    def __init__(self, size=5):
        """
        Cr√©e un environnement grille 5x5 avec :
        - Agent (A) : position de d√©part
        - Tr√©sor (T) : objectif (+10 points)
        - Obstacles (X) : p√©nalit√© (-5 points)
        - Cases vides (.) : p√©nalit√© l√©g√®re (-0.1 points)
        """
        self.size = size
        self.reset()
        
        # D√©finir les r√©compenses
        self.rewards = {
            'treasure': 10,      # Tr√©sor trouv√©
            'obstacle': -5,      # Obstacle touch√©
            'empty': -0.1,       # Case vide (pour encourager la rapidit√©)
            'wall': -1           # Essayer de sortir de la grille
        }
        
        # Actions possibles : 0=haut, 1=droite, 2=bas, 3=gauche
        self.actions = ['‚Üë', '‚Üí', '‚Üì', '‚Üê']
        self.action_effects = [(-1, 0), (0, 1), (1, 0), (0, -1)]
    
    def reset(self):
        """Remet l'environnement √† z√©ro"""
        # Cr√©er la grille
        self.grid = np.full((self.size, self.size), '.', dtype=str)
        
        # Position de d√©part de l'agent (coin haut-gauche)
        self.agent_pos = [0, 0]
        self.start_pos = [0, 0]
        
        # Position du tr√©sor (coin bas-droite)
        self.treasure_pos = [self.size-1, self.size-1]
        self.grid[self.treasure_pos[0], self.treasure_pos[1]] = 'T'
        
        # Placer quelques obstacles al√©atoirement
        num_obstacles = 3
        for _ in range(num_obstacles):
            while True:
                row, col = random.randint(0, self.size-1), random.randint(0, self.size-1)
                if [row, col] not in [self.start_pos, self.treasure_pos]:
                    self.grid[row, col] = 'X'
                    break
        
        return self.get_state()
    
    def get_state(self):
        """Retourne l'√©tat actuel (position de l'agent)"""
        return tuple(self.agent_pos)
    
    def step(self, action):
        """
        Ex√©cute une action et retourne (nouvel_√©tat, r√©compense, termin√©)
        """
        # Calculer la nouvelle position
        dr, dc = self.action_effects[action]
        new_row = self.agent_pos[0] + dr
        new_col = self.agent_pos[1] + dc
        
        # V√©rifier les limites de la grille
        if new_row < 0 or new_row >= self.size or new_col < 0 or new_col >= self.size:
            # Sortie de grille : p√©nalit√© mais pas de d√©placement
            return self.get_state(), self.rewards['wall'], False
        
        # D√©placer l'agent
        self.agent_pos = [new_row, new_col]
        current_cell = self.grid[new_row, new_col]
        
        # Calculer la r√©compense et v√©rifier si termin√©
        if current_cell == 'T':
            # Tr√©sor trouv√© !
            reward = self.rewards['treasure']
            done = True
        elif current_cell == 'X':
            # Obstacle touch√©
            reward = self.rewards['obstacle']
            done = True  # Episode termin√© (√©chec)
        else:
            # Case vide
            reward = self.rewards['empty']
            done = False
        
        return self.get_state(), reward, done
    
    def render(self):
        """Affiche l'√©tat actuel de la grille"""
        display_grid = self.grid.copy()
        agent_row, agent_col = self.agent_pos
        display_grid[agent_row, agent_col] = 'A'
        
        print("\n" + "="*20)
        for row in display_grid:
            print(" ".join(row))
        print("="*20)
        print("A=Agent, T=Tr√©sor, X=Obstacle, .=Vide")

# =========================================
# PARTIE 2 : AGENT Q-LEARNING
# =========================================

class QLearningAgent:
    def __init__(self, num_actions=4, learning_rate=0.1, discount_factor=0.9, epsilon=0.1):
        """
        Agent Q-Learning
        
        Args:
            num_actions: Nombre d'actions possibles
            learning_rate (Œ±): Taux d'apprentissage [0,1]
            discount_factor (Œ≥): Facteur d'actualisation [0,1] 
            epsilon (Œµ): Probabilit√© d'exploration [0,1]
        """
        self.num_actions = num_actions
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        
        # Table Q : Q(√©tat, action) -> valeur attendue
        self.q_table = defaultdict(lambda: np.zeros(num_actions))
        
        # Statistiques pour le suivi
        self.episode_rewards = []
        self.episode_steps = []
    
    def choose_action(self, state):
        """
        Choisit une action selon la politique Œµ-greedy
        
        Œµ-greedy : avec probabilit√© Œµ, explorer (action al√©atoire)
                   sinon, exploiter (meilleure action connue)
        """
        if random.random() < self.epsilon:
            # Exploration : action al√©atoire
            return random.randint(0, self.num_actions - 1)
        else:
            # Exploitation : meilleure action selon Q-table
            return np.argmax(self.q_table[state])
    
    def update_q_value(self, state, action, reward, next_state):
        """
        Met √† jour la valeur Q selon l'√©quation de Bellman :
        
        Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥¬∑max(Q(s',a')) - Q(s,a)]
        
        o√π :
        - Œ± = learning_rate
        - Œ≥ = discount_factor  
        - r = reward
        - s = state, a = action
        - s' = next_state
        """
        current_q = self.q_table[state][action]
        max_next_q = np.max(self.q_table[next_state])
        
        # √âquation de Bellman
        new_q = current_q + self.learning_rate * (
            reward + self.discount_factor * max_next_q - current_q
        )
        
        self.q_table[state][action] = new_q
    
    def decay_epsilon(self, decay_rate=0.995, min_epsilon=0.01):
        """R√©duit progressivement l'exploration"""
        self.epsilon = max(min_epsilon, self.epsilon * decay_rate)

# =========================================
# PARTIE 3 : ENTRA√éNEMENT
# =========================================

def train_agent(env, agent, num_episodes=1000, verbose=True):
    """
    Entra√Æne l'agent dans l'environnement
    """
    print(f"üöÄ D√âBUT DE L'ENTRA√éNEMENT ({num_episodes} √©pisodes)")
    print("="*50)
    
    all_rewards = []
    all_steps = []
    
    for episode in range(num_episodes):
        # R√©initialiser l'environnement
        state = env.reset()
        total_reward = 0
        steps = 0
        done = False
        
        while not done and steps < 100:  # Limite pour √©viter les boucles infinies
            # Choisir une action
            action = agent.choose_action(state)
            
            # Ex√©cuter l'action
            next_state, reward, done = env.step(action)
            
            # Mettre √† jour la Q-table
            agent.update_q_value(state, action, reward, next_state)
            
            # Mise √† jour pour la prochaine it√©ration
            state = next_state
            total_reward += reward
            steps += 1
        
        # Enregistrer les statistiques
        all_rewards.append(total_reward)
        all_steps.append(steps)
        
        # R√©duire l'exploration progressivement
        agent.decay_epsilon()
        
        # Affichage p√©riodique
        if verbose and (episode + 1) % 100 == 0:
            avg_reward = np.mean(all_rewards[-100:])
            avg_steps = np.mean(all_steps[-100:])
            print(f"√âpisode {episode+1:4d}: R√©compense moy.={avg_reward:6.2f}, "
                  f"√âtapes moy.={avg_steps:5.1f}, Œµ={agent.epsilon:.3f}")
    
    return all_rewards, all_steps

# =========================================
# PARTIE 4 : TEST DE L'AGENT ENTRA√éN√â
# =========================================

def test_agent(env, agent, num_tests=5):
    """
    Teste l'agent entra√Æn√© (pas d'apprentissage, pas d'exploration)
    """
    print(f"\nüéØ TEST DE L'AGENT ENTRA√éN√â ({num_tests} tests)")
    print("="*50)
    
    # Sauvegarder l'epsilon original
    original_epsilon = agent.epsilon
    agent.epsilon = 0  # Pas d'exploration pendant le test
    
    successes = 0
    
    for test in range(num_tests):
        state = env.reset()
        total_reward = 0
        steps = 0
        done = False
        path = [state]
        
        print(f"\n--- Test {test+1} ---")
        env.render()
        
        while not done and steps < 20:
            action = agent.choose_action(state)
            next_state, reward, done = env.step(action)
            
            path.append(next_state)
            total_reward += reward
            steps += 1
            state = next_state
            
            print(f"√âtape {steps}: Action={env.actions[action]} ‚Üí "
                  f"Position={next_state}, R√©compense={reward:.1f}")
        
        # R√©sultat du test
        if env.agent_pos == env.treasure_pos:
            print(f"üéâ SUCC√àS ! Tr√©sor trouv√© en {steps} √©tapes (R√©compense: {total_reward:.1f})")
            successes += 1
        else:
            print(f"‚ùå √âchec. Position finale: {env.agent_pos} (R√©compense: {total_reward:.1f})")
        
        env.render()
    
    # Restaurer epsilon
    agent.epsilon = original_epsilon
    
    print(f"\nüìä R√âSULTATS: {successes}/{num_tests} succ√®s ({100*successes/num_tests:.1f}%)")

# =========================================
# PARTIE 5 : VISUALISATION DES R√âSULTATS
# =========================================

def plot_training_results(rewards, steps):
    """
    Graphiques des r√©sultats d'entra√Ænement
    """
    # Calculer les moyennes mobiles
    window = 50
    if len(rewards) >= window:
        rewards_smooth = np.convolve(rewards, np.ones(window)/window, mode='valid')
        steps_smooth = np.convolve(steps, np.ones(window)/window, mode='valid')
        x_smooth = range(window-1, len(rewards))
    else:
        rewards_smooth = rewards
        steps_smooth = steps
        x_smooth = range(len(rewards))
    
    # Cr√©er les graphiques
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # Graphique des r√©compenses
    ax1.plot(rewards, alpha=0.3, color='blue', label='R√©compenses brutes')
    if len(rewards_smooth) > 1:
        ax1.plot(x_smooth, rewards_smooth, color='red', linewidth=2, label=f'Moyenne mobile ({window})')
    ax1.set_xlabel('√âpisodes')
    ax1.set_ylabel('R√©compense totale')
    ax1.set_title('√âvolution des r√©compenses')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Graphique du nombre d'√©tapes
    ax2.plot(steps, alpha=0.3, color='green', label='√âtapes brutes')
    if len(steps_smooth) > 1:
        ax2.plot(x_smooth, steps_smooth, color='orange', linewidth=2, label=f'Moyenne mobile ({window})')
    ax2.set_xlabel('√âpisodes')
    ax2.set_ylabel('Nombre d\'√©tapes')
    ax2.set_title('√âvolution du nombre d\'√©tapes')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

def visualize_q_table(agent, env):
    """
    Visualise la politique apprise (meilleure action par √©tat)
    """
    print(f"\nüß† POLITIQUE APPRISE (meilleure action par √©tat)")
    print("="*50)
    
    policy_grid = np.full((env.size, env.size), '?', dtype=str)
    
    for row in range(env.size):
        for col in range(env.size):
            state = (row, col)
            if state in agent.q_table:
                best_action = np.argmax(agent.q_table[state])
                policy_grid[row, col] = env.actions[best_action]
    
    # Marquer les positions sp√©ciales
    policy_grid[env.start_pos[0], env.start_pos[1]] = 'S'  # Start
    policy_grid[env.treasure_pos[0], env.treasure_pos[1]] = 'T'  # Treasure
    
    # Marquer les obstacles
    for row in range(env.size):
        for col in range(env.size):
            if env.grid[row, col] == 'X':
                policy_grid[row, col] = 'X'
    
    print("Politique (meilleure action par case):")
    for row in policy_grid:
        print(" ".join(f"{cell:>2}" for cell in row))
    print("\nS=Start, T=Treasure, X=Obstacle, ‚Üë‚Üí‚Üì‚Üê=Directions recommand√©es")

# =========================================
# PARTIE 6 : D√âMONSTRATION COMPL√àTE
# =========================================

def main_demonstration():
    """
    D√©monstration compl√®te de l'apprentissage par renforcement
    """
    print("ü§ñ APPRENTISSAGE PAR RENFORCEMENT - Q-LEARNING")
    print("=" * 60)
    
    # 1. Cr√©er l'environnement
    print("\n1Ô∏è‚É£ CR√âATION DE L'ENVIRONNEMENT")
    env = GridWorld(size=5)
    env.render()
    
    # 2. Cr√©er l'agent
    print("\n2Ô∏è‚É£ CR√âATION DE L'AGENT Q-LEARNING")
    agent = QLearningAgent(
        num_actions=4,
        learning_rate=0.1,    # Œ± : vitesse d'apprentissage
        discount_factor=0.9,  # Œ≥ : importance du futur
        epsilon=0.3           # Œµ : taux d'exploration initial
    )
    
    print(f"Agent cr√©√© avec :")
    print(f"  - Taux d'apprentissage (Œ±): {agent.learning_rate}")
    print(f"  - Facteur d'actualisation (Œ≥): {agent.discount_factor}")
    print(f"  - Taux d'exploration initial (Œµ): {agent.epsilon}")
    
    # 3. Entra√Ænement
    print("\n3Ô∏è‚É£ ENTRA√éNEMENT")
    rewards, steps = train_agent(env, agent, num_episodes=500, verbose=True)
    
    # 4. Visualisation des r√©sultats
    print("\n4Ô∏è‚É£ VISUALISATION DES R√âSULTATS")
    try:
        plot_training_results(rewards, steps)
    except:
        print("Matplotlib non disponible pour les graphiques")
    
    # 5. Test de l'agent entra√Æn√©
    print("\n5Ô∏è‚É£ TEST DE L'AGENT ENTRA√éN√â")
    test_agent(env, agent, num_tests=3)
    
    # 6. Visualisation de la politique
    print("\n6Ô∏è‚É£ POLITIQUE APPRISE")
    visualize_q_table(agent, env)
    
    # 7. Statistiques finales
    print(f"\nüìà STATISTIQUES FINALES")
    print("="*30)
    print(f"√âtats explor√©s: {len(agent.q_table)}")
    print(f"R√©compense moyenne (100 derniers √©pisodes): {np.mean(rewards[-100:]):.2f}")
    print(f"√âtapes moyennes (100 derniers √©pisodes): {np.mean(steps[-100:]):.1f}")
    print(f"Epsilon final: {agent.epsilon:.3f}")

# =========================================
# EXERCICES PRATIQUES
# =========================================

def exercices_supplementaires():
    """
    Exercices pour approfondir la compr√©hension
    """
    print(f"\nüéì EXERCICES POUR APPROFONDIR")
    print("="*40)
    
    exercises = [
        "1. Modifier les r√©compenses et observer l'impact sur l'apprentissage",
        "2. Changer la taille de la grille (3x3, 7x7) et comparer",
        "3. Tester diff√©rents param√®tres (Œ±, Œ≥, Œµ) et analyser",
        "4. Ajouter plus d'obstacles et voir comment l'agent s'adapte",
        "5. Impl√©menter SARSA au lieu de Q-Learning",
        "6. Ajouter des √©tats partiellement observables",
        "7. Cr√©er un environnement plus complexe (labyrinthe)",
        "8. Impl√©menter Double Q-Learning pour √©viter la surestimation"
    ]
    
    for exercise in exercises:
        print(f"   {exercise}")
    
    print(f"\nüí° Conseils :")
    print("   - Commencez par modifier un param√®tre √† la fois")
    print("   - Observez les courbes d'apprentissage")  
    print("   - Testez la robustesse avec diff√©rents seeds al√©atoires")

# =========================================
# EX√âCUTION DU PROGRAMME PRINCIPAL
# =========================================

if __name__ == "__main__":
    # Fixer la graine pour la reproductibilit√©
    random.seed(42)
    np.random.seed(42)
    
    # Lancer la d√©monstration
    main_demonstration()
    
    # Proposer des exercices
    # exercices_supplementaires()
    
    print(f"\nüéâ D√âMONSTRATION TERMIN√âE !")
    print("="*30)